<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multiclass Classifier Training Agent - WhiteLightning</title>
    <link rel="icon" type="image/png" href="media/image/favicons-f07016ef02d695ba85dee5ffd3b42f39(pr-cy.ru)/favicon-32x32.png" />
    <link rel="stylesheet" href="styles/running.css">
</head>
<body>
    <header class="wl-header">
        <div class="logo">
          <img src="media/image/moonshiner_floppy.jpeg" alt="WhiteLightning Logo">
          <h1><a href="index.html" style="text-decoration: none;letter-spacing: 0px; color: inherit; ">White<span>Lightning</span></a></h1>
        </div>
        <div class="hamburger">
          <span></span>
          <span></span>
          <span></span>
        </div>
        <nav class="desktop-nav">
          <ul>
            <li class="dropdown">
              <a href="#">Demos</a>
              <ul class="dropdown-menu">
                <li><a href="examples.html">Promt Examples</a></li>
                <li><a href="demo_mlt.html">Mobile Deployment</a></li>
              </ul>
            </li>
            <li class="dropdown">
              <a href="#">Documentation</a>
              <ul class="dropdown-menu">
                <li><a href="multiclass-classifier.html">Multiclass Setup</a></li>
                <li><a href="binary-classifier.html">Binary Setup</a></li>
                <li><a id="plgr_btn" href="running_mlt.html">Multiclass Running</a></li>
                <li><a href="running_bnr.html">Binary Running</a></li>
              </ul>
            </li>
            <li><a  href="playground.html">Playground</a></li>
            <li><a  href="docs.html">Getting Started</a></li>
            <li><a href="https://github.com/volodymyrparanyak/whitelightning.ai" target="_blank">GitHub</a></li>
          </ul>
        </nav>
        <nav class="mobile-nav">
          <ul>
            <li><a  href="examples.html">Promt Examples</a></li>
            <li><a  href="binary-classifier.html">Binary Setup</a></li>
            <li><a  href="running_bnr.html">Binary Running </a></li>
            <li><a  href="multiclass-classifier.html">Multiclass Setup</a></li>
            <li><a id="plgr_btn" href="running_mlt.html">Multiclass Running</a></li>
            <li><a  href="demo_mlt.html">Mobile Deployment</a></li>
            <li><a  href="playground.html">Playground</a></li>
            <li><a  href="docs.html">Getting Started</a></li>
          </ul>
        </nav>
      </header>

    <div class="docs-container">
        <!-- <div class="docs-sidebar">
            <nav class="docs-nav">
              <ul>
                <li><a href="docs.html">Overview</a>
                  <ul style="margin-left:1.2em;">
                    <li><a href="docs.html#what-is-llm">What is LLM Distillation?</a></li>
                    <li><a href="docs.html#why-onnx">Why ONNX?</a></li>
                    <li><a href="docs.html#features">Key Features</a></li>
                    <li><a href="docs.html#quick-start">Quick Start</a></li>
                    <li><a href="docs.html#documentation">Deployment Guide</a></li>
                  </ul>
                </li>
                <li><a href="#model-types">Model Types</a>
                  <ul style="margin-left:1.2em;">
                    <li><a href="binary-classifier.html">Binary Classifier</a></li>
                    <ul style="margin-left:1.2em;">
                        <li><a href="running_bnr.html">Running</a></li>
                      </ul>
                    <li><a href="multiclass-classifier.html">Multiclass Classifier</a>
                      <ul style="margin-left:1.2em;">
                        <li><a href="running_mlt.html" class="active">Running</a></li>
                      </ul>
                      <ul style="margin-left:1.2em;">
                        <li><a href="demo_mlt.html">Demos</a></li>
                      </ul>
                    </li>
                  </ul>
                </li>
               
              </ul>
            </nav>
          </div> -->

        <main class="docs-content">
            <section id="running-multiclass-models">
                <h1 class="neon-green">Running Multiclass Classifier Models</h1>
                <div class="terminal-container intro-terminal">
                    <div class="terminal-header">
                        <div class="terminal-buttons">
                            <span class="terminal-circle red"></span>
                            <span class="terminal-circle yellow"></span>
                            <span class="terminal-circle green"></span>
                        </div>
                        <div class="terminal-title">WhiteLightning.ai Intro</div>
                    </div>
                    <div class="terminal-intro"><span id="typewriter-intro"></span></div>
                </div>
                
                <div class="process-cards">
                    <div class="process-card">
                        <h2 class="neon-green">üîÑ Preprocessing: Tokenizing the Text</h2>
                        <div class="process-steps">
                            <div class="step">
                                <span class="step-number">1</span>
                                <div class="step-content">
                                    <h4>Text Input</h4>
                                    <p>Start with a string (e.g., "The government announced new policies")</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">2</span>
                                <div class="step-content">
                                    <h4>Tokenization</h4>
                                    <p>Convert words to lowercase and map them to integer IDs using a tokenizer vocabulary (from <code>_tokenizer.json</code>). Unknown words use the <code>&lt;OOV&gt;</code> token (default ID: 1)</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">3</span>
                                <div class="step-content">
                                    <h4>Padding/Truncation</h4>
                                    <p>Truncate to 30 tokens and pad with zeros to ensure a fixed-length sequence</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">4</span>
                                <div class="step-content">
                                    <h4>Output</h4>
                                    <p>A 30-element <code>int32</code> array, ready for the ONNX model</p>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="process-card">
                        <h2 class="neon-green">ü§ñ Inference: Classifying the Text</h2>
                        <div class="process-steps">
                            <div class="step">
                                <span class="step-number">1</span>
                                <div class="step-content">
                                    <h4>Model Loading</h4>
                                    <p>Load the ONNX model (<code>news_classifier.onnx</code>) using the ONNX Runtime</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">2</span>
                                <div class="step-content">
                                    <h4>Inference</h4>
                                    <p>Pass the <code>[1, 30]</code> <code>int32</code> tensor to the model, which outputs a softmax probability distribution over classes</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">3</span>
                                <div class="step-content">
                                    <h4>Label Mapping</h4>
                                    <p>Load the label map (from <code>_scaler.json</code>) to convert the highest-probability index to a class name (e.g., "Politics")</p>
                                </div>
                            </div>
                            <div class="step">
                                <span class="step-number">4</span>
                                <div class="step-content">
                                    <h4>Output</h4>
                                    <p>The predicted class and its probability score (e.g., "Politics (Score: 0.9123)")</p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <!-- <section id="onnx-demos">
                <h2 class="neon-green">üì± ONNX Model Deployment Demos</h2>
                <h3 class="neon-green">ü§ñ Android</h3>
                <p>üé¨ <strong>Demo: ONNX inference running on Android device</strong></p>
                <p align="center">
                   <img src="media/Android runningONNX.gif" width="200" alt="CLI Usage">
                </p>
                <p>üîó <a class="neon-link" href="https://www.youtube.com/watch?v=ANDROID_VIDEO_ID" target="_blank">Watch Full Video on YouTube</a></p>
                <h3 class="neon-green">üçé iOS</h3>
                <p>üé¨ <strong>Demo: ONNX model working in an iOS app (Swift + CoreML)</strong></p>
                <p align="center">
                   <img src="media/OIS runningONNX.gif" width="700" alt="CLI Usage">
                </p>
                <p>üîó <a class="neon-link" href="https://www.youtube.com/watch?v=IOS_VIDEO_ID" target="_blank">Watch iOS Deployment on YouTube</a></p>
            </section> -->

            <section id="code-examples">
                <h2 class="neon-green">üíª Running Guide</h2>
                <div class="terminal-container">
                    <div class="terminal-header">
                        <div class="terminal-buttons">
                            <span class="terminal-circle red"></span>
                            <span class="terminal-circle yellow"></span>
                            <span class="terminal-circle green"></span>
                        </div>
                        <div class="terminal-title"></div>
                        <button class="copy-btn" title="Copy code">Copy</button>
                    </div>
                    <div class="terminal-intro"><span id="typewriter"></span></div>
                    <div class="language-selector">
                        <button class="lang-btn active" data-lang="python">Python</button>
                        <button class="lang-btn" data-lang="javascript">JavaScript</button>
                        <button class="lang-btn" data-lang="c">C</button>
                        <button class="lang-btn" data-lang="cpp">C++</button>
                        <button class="lang-btn" data-lang="rust">Rust</button>
                        <button class="lang-btn" data-lang="java">Java</button>
                    </div>
                    <div class="terminal-content">
                        <div class="code-block active" id="python-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with Python: Full Beginner-Friendly Guide</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide walks you through running an ONNX model for text classification using Python, starting from scratch ‚Äî including Python installation, setting up dependencies, and running the model.</p>

                                <h4>‚úÖ 1. Install Python</h4>
                                <div class="os-section">
                                    <h5>üî∑ Windows</h5>
                                    <ol>
                                        <li>Go to: <a href="https://www.python.org/downloads/windows" class="neon-link">https://www.python.org/downloads/windows</a></li>
                                        <li>Download the latest Python 3.11+ installer</li>
                                        <li>During installation, check ‚úÖ Add Python to PATH</li>
                                        <li>After installation, check if it worked:</li>
                                    </ol>
                                    <pre><code>python --version</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üçè macOS</h5>
                                    <p>You have two options to install Python:</p>
                                    <ol>
                                        <li><strong>Option 1 - Official Website:</strong>
                                            <ul>
                                                <li>Visit: <a href="https://www.python.org/downloads/macos/" class="neon-link">https://www.python.org/downloads/macos/</a></li>
                                                <li>Download the latest Python 3.11+ installer for macOS</li>
                                                <li>Run the installer package and follow the installation wizard</li>
                                            </ul>
                                        </li>
                                        <li><strong>Option 2 - Homebrew:</strong>
                                            <ul>
                                                <li>Install Homebrew (if you don't have it):</li>
                                            </ul>
                                            <pre><code>/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"</code></pre>
                                            <p>Then install Python:</p>
                                            <pre><code>brew install python@3.11</code></pre>
                                        </li>
                                    </ol>
                                    <p>After installation, check if it worked:</p>
                                    <pre><code>python3 --version</code></pre>
                                    <p><em>Note: macOS uses python3, not python.</em></p>
                                </div>

                                <div class="os-section">
                                    <h5>üêß Linux (Ubuntu/Debian)</h5>
                                    <p>You have two options to install Python:</p>
                                    <ol>
                                        <li><strong>Option 1 - Package Manager:</strong>
                                            <pre><code>sudo apt update
sudo apt install python3 python3-pip</code></pre>
                                        </li>
                                        <li><strong>Option 2 - Official Website:</strong>
                                            <ul>
                                                <li>Visit: <a href="https://www.python.org/downloads/source/" class="neon-link">https://www.python.org/downloads/source/</a></li>
                                                <li>Download the latest Python 3.11+ source tarball</li>
                                                <li>Extract and build from source:</li>
                                            </ul>
                                            <pre><code>tar -xf Python-3.11.x.tar.xz
cd Python-3.11.x
./configure
make
sudo make install</code></pre>
                                        </li>
                                    </ol>
                                    <p>After installation, check if it worked:</p>
                                    <pre><code>python3 --version</code></pre>
                                </div>

                                <h4>‚úÖ 2. Get Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 3. Set Up Your Project Folder</h4>
                                <pre><code>mkdir onnx_python_demo
cd onnx_python_demo</code></pre>
                                <p>Folder structure:</p>
                                <pre><code>onnx_python_demo/
‚îú‚îÄ‚îÄ model.onnx
‚îú‚îÄ‚îÄ vocab.json
‚îú‚îÄ‚îÄ scaler.json
‚îî‚îÄ‚îÄ run_onnx.py</code></pre>

                                <h4>‚úÖ 4. Install Required Python Libraries</h4>
                                <pre><code>pip install onnxruntime numpy</code></pre>
                                <p><em>On macOS or Linux, you might need to run pip3 install instead.</em></p>

                                <h4>‚úÖ 5. Prepare Supporting Files</h4>
                                <div class="file-section">
                                    <h5>üîπ vocab.json (tokenizer dictionary)</h5>
                                    <pre><code>{
  "<OOV>": 1,
  "the": 2,
  "government": 3,
  "announced": 4,
  "new": 5,
  "policies": 6,
  "to": 7,
  "boost": 8,
  "economy": 9
}</code></pre>
                                </div>

                                <div class="file-section">
                                    <h5>üîπ scaler.json (label map)</h5>
                                    <pre><code>{
  "0": "Politics",
  "1": "Sports",
  "2": "Technology"
}</code></pre>
                                    <p><em>Adjust to match your model's output classes.</em></p>
                                </div>

                                <div class="file-section">
                                    <h5>üîπ model.onnx</h5>
                                    <p>Place your trained ONNX model here. It should accept a (1, 30) input tensor of int32.</p>
                                </div>

                                <h4>‚úÖ 6. Create the Python Script run_onnx.py</h4>
                                <p>Use the code example below:</p>
                            </div>
                            <pre><code><span class="keyword">import</span> <span class="module">json</span>
<span class="keyword">import</span> <span class="module">numpy</span> <span class="keyword">as</span> <span class="module">np</span>
<span class="keyword">import</span> <span class="module">onnxruntime</span> <span class="keyword">as</span> <span class="module">ort</span>

<span class="keyword">def</span> <span class="function">preprocess_text</span>(<span class="parameter">text</span>, <span class="parameter">tokenizer_file</span>):
    <span class="keyword">with</span> <span class="function">open</span>(tokenizer_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:
        tokenizer = <span class="module">json</span>.<span class="function">load</span>(f)
    
    oov_token = <span class="string">'&lt;OOV&gt;'</span>
    words = text.<span class="function">lower</span>().<span class="function">split</span>()
    sequence = [tokenizer.<span class="function">get</span>(word, tokenizer.<span class="function">get</span>(oov_token, <span class="number">1</span>)) <span class="keyword">for</span> word <span class="keyword">in</span> words]
    sequence = sequence[:<span class="number">30</span>]  <span class="comment"># Truncate to max_len</span>
    padded = np.<span class="function">zeros</span>(<span class="number">30</span>, dtype=np.int32)
    padded[:<span class="function">len</span>(sequence)] = sequence  <span class="comment"># Pad with zeros</span>
    <span class="keyword">return</span> padded

<span class="comment"># Test</span>
text = <span class="string">"The government announced new policies to boost the economy"</span>
vector = <span class="function">preprocess_text</span>(text, <span class="string">'vocab.json'</span>)

session = ort.<span class="class">InferenceSession</span>(<span class="string">'model.onnx'</span>)
input_name = session.<span class="function">get_inputs</span>()[<span class="number">0</span>].name
output_name = session.<span class="function">get_outputs</span>()[<span class="number">0</span>].name
input_data = vector.<span class="function">reshape</span>(<span class="number">1</span>, <span class="number">30</span>)
outputs = session.<span class="function">run</span>([output_name], {input_name: input_data})

<span class="comment"># Load label map</span>
<span class="keyword">with</span> <span class="function">open</span>(<span class="string">'scaler.json'</span>, <span class="string">'r'</span>) <span class="keyword">as</span> f:
    label_map = <span class="module">json</span>.<span class="function">load</span>(f)

probabilities = outputs[<span class="number">0</span>][<span class="number">0</span>]
predicted_idx = np.<span class="function">argmax</span>(probabilities)
label = label_map[<span class="function">str</span>(predicted_idx)]
score = probabilities[predicted_idx]
<span class="function">print</span>(<span class="string">f'Python ONNX output: {label} (Score: {score:.4f})'</span>)

</code></pre>

                            <div class="guide-section">
                                <h4>‚úÖ 7. Run the Script</h4>
                                <div class="os-section">
                                    <h5>üî∑ Windows</h5>
                                    <pre><code>python run_onnx.py</code></pre>
                                </div>
                                <div class="os-section">
                                    <h5>üçè macOS/Linux</h5>
                                    <pre><code>python3 run_onnx.py</code></pre>
                                </div>

                                <h4>‚úÖ 8. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>Prediction: Score: 0.9732</code></pre>
                                    <p>The output shows the predicted class and its confidence score. In this example, the model predicted "Politics" with a confidence of 97.32%.</p>
                                </div>
                            </div>
                        </div>
                        <div class="code-block" id="javascript-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with JavaScript (Browser or Node.js)</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide explains how to load and run ONNX models using JavaScript and ONNX Runtime Web, covering both browser and Node.js environments.</p>

                                <h4>‚úÖ 1. Choose Your Runtime</h4>
                                <p>You can run ONNX models in JavaScript in two ways:</p>
                                <table>
                                    <tr>
                                        <th>Environment</th>
                                        <th>Description</th>
                                        <th>Recommended For</th>
                                    </tr>
                                    <tr>
                                        <td>‚úÖ Browser</td>
                                        <td>Uses WebAssembly or WebGL</td>
                                        <td>Web apps, frontend demos</td>
                                    </tr>
                                    <tr>
                                        <td>‚úÖ Node.js</td>
                                        <td>Uses Node runtime (CPU only)</td>
                                        <td>Backend/CLI usage</td>
                                    </tr>
                                </table>

                                <h4>‚úÖ 2. Requirements</h4>
                                <div class="os-section">
                                    <h5>üî∑ For browser</h5>
                                    <p>No install ‚Äî just include the library from a CDN or bundle via npm.</p>
                                </div>

                                <div class="os-section">
                                    <h5>üü© For Node.js</h5>
                                    <p>Install Node.js:</p>
                                    <ol>
                                        <li>Download from: <a href="https://nodejs.org/" class="neon-link">https://nodejs.org/</a></li>
                                        <li>Check installation:</li>
                                    </ol>
                                    <pre><code>node -v
npm -v</code></pre>
                                    <p>Then install ONNX Runtime:</p>
                                    <pre><code>npm install onnxruntime-web</code></pre>
                                </div>

                                <h4>‚úÖ 3. Get Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 4. Folder Setup</h4>
                                <pre><code>mkdir onnx_js_demo
cd onnx_js_demo</code></pre>
                                <p>Files you'll need:</p>
                                <pre><code>onnx_js_demo/
‚îú‚îÄ‚îÄ index.html            # For browser use
‚îú‚îÄ‚îÄ run.js                # Main logic
‚îú‚îÄ‚îÄ model.onnx
‚îú‚îÄ‚îÄ vocab.json
‚îú‚îÄ‚îÄ scaler.json</code></pre>

                                <h4>‚úÖ 5. Sample vocab.json</h4>
                                <pre><code>{
  "<OOV>": 1,
  "the": 2,
  "government": 3,
  "announced": 4,
  "new": 5,
  "policies": 6,
  "to": 7,
  "boost": 8,
  "economy": 9
}</code></pre>

                                <h4>‚úÖ 6. Sample scaler.json</h4>
                                <pre><code>{
  "0": "Politics",
  "1": "Sports",
  "2": "Technology"
}</code></pre>

                                <h4>‚úÖ 7. JavaScript Code (run.js)</h4>
                                <p>Works in both browser and Node.js (with minor changes)</p>
                                <pre><code><span class="keyword">async</span> <span class="keyword">function</span> <span class="function">preprocessText</span>(<span class="parameter">text</span>, <span class="parameter">tokenizerUrl</span>) {
<span class="keyword">const</span> tokenizerResp = <span class="keyword">await</span> <span class="function">fetch</span>(tokenizerUrl);
<span class="keyword">const</span> tokenizer = <span class="keyword">await</span> tokenizerResp.<span class="function">json</span>();

<span class="keyword">const</span> oovToken = <span class="string">'&lt;OOV&gt;'</span>;
<span class="keyword">const</span> words = text.<span class="function">toLowerCase</span>().<span class="function">split</span>(<span class="regex">/\s+/</span>);
<span class="keyword">const</span> sequence = words.<span class="function">map</span>(word => tokenizer[word] || tokenizer[oovToken] || <span class="number">1</span>).<span class="function">slice</span>(<span class="number">0</span>, <span class="number">30</span>);
<span class="keyword">const</span> padded = <span class="keyword">new</span> <span class="class">Int32Array</span>(<span class="number">30</span>).<span class="function">fill</span>(<span class="number">0</span>);
sequence.<span class="function">forEach</span>((val, idx) => padded[idx] = val);
<span class="keyword">return</span> padded;
}

<span class="keyword">async</span> <span class="keyword">function</span> <span class="function">runModel</span>(<span class="parameter">text</span>) {
<span class="keyword">const</span> session = <span class="keyword">await</span> ort.<span class="class">InferenceSession</span>.<span class="function">create</span>(<span class="string">'model.onnx'</span>);
<span class="keyword">const</span> vector = <span class="keyword">await</span> <span class="function">preprocessText</span>(text, <span class="string">'vocab.json'</span>);
<span class="keyword">const</span> tensor = <span class="keyword">new</span> ort.<span class="class">Tensor</span>(<span class="string">'int32'</span>, vector, [<span class="number">1</span>, <span class="number">30</span>]);
<span class="keyword">const</span> feeds = { input: tensor };
<span class="keyword">const</span> output = <span class="keyword">await</span> session.<span class="function">run</span>(feeds);

<span class="keyword">const</span> labelResp = <span class="keyword">await</span> <span class="function">fetch</span>(<span class="string">'scaler.json'</span>);
<span class="keyword">const</span> labelMap = <span class="keyword">await</span> labelResp.<span class="function">json</span>();

<span class="keyword">const</span> probabilities = output[<span class="class">Object</span>.<span class="function">keys</span>(output)[<span class="number">0</span>]].data;
<span class="keyword">const</span> predictedIdx = probabilities.<span class="function">reduce</span>((maxIdx, val, idx) => val > probabilities[maxIdx] ? idx : maxIdx, <span class="number">0</span>);
<span class="keyword">const</span> label = labelMap[predictedIdx];
<span class="keyword">const</span> score = probabilities[predictedIdx];
<span class="function">console</span>.<span class="function">log</span>(<span class="string">`JS ONNX output: ${label} (Score: ${score.toFixed(4)})`</span>);
}

<span class="function">runModel</span>(<span class="string">'The government announced new policies to boost the economy'</span>);</code></pre>
                            

                                <h4>‚úÖ 8. Run in Browser (option A)</h4>
                                <div class="file-section">
                                    <h5>üîπ index.html</h5>
                                    <pre><code>&lt;!DOCTYPE html&gt;
&lt;html lang="en"&gt;
&lt;head&gt;
  &lt;meta charset="UTF-8" /&gt;
  &lt;title&gt;ONNX JS Inference&lt;/title&gt;
&lt;/head&gt;
&lt;body&gt;
  &lt;h1&gt;Running ONNX Model...&lt;/h1&gt;
  &lt;script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"&gt;&lt;/script&gt;
  &lt;script type="module" src="run.js"&gt;&lt;/script&gt;
&lt;/body&gt;
&lt;/html&gt;</code></pre>
                                </div>
                                <p>üì¶ Start a local server (required due to fetch):</p>
                                <pre><code>npx serve .
# OR
python3 -m http.server</code></pre>
                                <p>Visit: http://localhost:3000</p>

                                <h4>‚úÖ 9. Run with Node.js (option B)</h4>
                                <div class="file-section">
                                    <h5>üîπ Modify run.js for Node</h5>
                                    <pre><code><span class="keyword">import</span> * <span class="keyword">as</span> ort <span class="keyword">from</span> <span class="string">'onnxruntime-node'</span>;
<span class="keyword">import</span> fs <span class="keyword">from</span> <span class="string">'fs/promises'</span>;

<span class="keyword">async function</span> <span class="function">loadJSON</span>(<span class="parameter">path</span>) {
  <span class="keyword">const</span> data = <span class="keyword">await</span> fs.<span class="function">readFile</span>(path, <span class="string">'utf-8'</span>);
  <span class="keyword">return</span> <span class="class">JSON</span>.<span class="function">parse</span>(data);
}

<span class="comment">// Keep rest of logic same from previous example</span></code></pre>
                                </div>
                                <p>üì¶ Run it:</p>
                                <pre><code>node run.js</code></pre>

                                <h4>‚úÖ 10. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>JS ONNX output: Score: 0.9732</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="code-block" id="c-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with C using ONNX Runtime and cJSON</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide explains how to load and run ONNX models using C, ONNX Runtime C API, and cJSON for JSON parsing.</p>

                                <h4>‚úÖ 1. Prerequisites</h4>
                                <div class="os-section">
                                    <h5>üî∑ C Compiler</h5>
                                    <p>macOS: clang comes with Xcode Command Line Tools</p>
                                    <pre><code>xcode-select --install</code></pre>
                                    <p>Linux: install gcc</p>
                                    <pre><code>sudo apt install build-essential</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üü© ONNX Runtime C Library</h5>
                                    <p>Download ONNX Runtime C API from the official website:</p>
                                    <p>üëâ <a href="https://github.com/microsoft/onnxruntime/releases" class="neon-link">https://github.com/microsoft/onnxruntime/releases</a></p>
                                    <p>Choose:</p>
                                    <pre><code>onnxruntime-osx-universal2-<version>.tgz   # For macOS
onnxruntime-linux-x64-<version>.tgz        # For Linux</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Install cJSON</h5>
                                    <p>macOS:</p>
                                    <pre><code>brew install cjson</code></pre>
                                    <p>Linux:</p>
                                    <pre><code>sudo apt install libcjson-dev</code></pre>
                                </div>

                                <h4>‚úÖ 2. Choose Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 3. Folder Structure</h4>
                                <pre><code>project/
‚îú‚îÄ‚îÄ ONNX_test.c               ‚Üê your C code
‚îú‚îÄ‚îÄ vocab.json                ‚Üê tokenizer
‚îú‚îÄ‚îÄ scaler.json               ‚Üê label map
‚îú‚îÄ‚îÄ model.onnx                ‚Üê ONNX model
‚îú‚îÄ‚îÄ onnxruntime-osx-universal2-1.22.0/
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ lib/</code></pre>

                                <h4>‚úÖ 4. Build Command</h4>
                                <div class="os-section">
                                    <h5>üî∑ macOS</h5>
                                    <pre><code>gcc ONNX_test.c \
  -I./onnxruntime-osx-universal2-1.22.0/include \
  -L./onnxruntime-osx-universal2-1.22.0/lib \
  -lonnxruntime \
  -lcjson \
  -o onnx_test</code></pre>
                                </div>
                                <div class="os-section">
                                    <h5>üêß Linux</h5>
                                    <p>Replace the onnxruntime-osx-... path with onnxruntime-linux-x64-....</p>
                                </div>

                                <h4>‚úÖ 5. Run the Executable</h4>
                                <div class="os-section">
                                    <h5>üî∑ macOS</h5>
                                    <p>Important: You must set the library path.</p>
                                    <pre><code>export DYLD_LIBRARY_PATH=./onnxruntime-osx-universal2-1.22.0/lib:$DYLD_LIBRARY_PATH
./onnx_test</code></pre>
                                </div>
                                <div class="os-section">
                                    <h5>üêß Linux</h5>
                                    <pre><code>export LD_LIBRARY_PATH=./onnxruntime-linux-x64-1.22.0/lib:$LD_LIBRARY_PATH
./onnx_test</code></pre>
                                </div>

                                <h4>‚úÖ 6. C Code Example</h4>
                                <pre><code><span class="keyword">#include</span> <span class="string">&lt;stdio.h&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;stdlib.h&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;string.h&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;ctype.h&gt;</span>
<span class="keyword">#include</span> <span class="string">"onnxruntime-osx-universal2-1.22.0/include/onnxruntime_c_api.h"</span>
<span class="keyword">#include</span> <span class="string">&lt;cjson/cJSON.h&gt;</span>

<span class="keyword">const</span> OrtApi* g_ort = NULL;

<span class="keyword">int32_t*</span> <span class="function">preprocess_text</span>(<span class="parameter">const char* text</span>, <span class="parameter">const char* tokenizer_file</span>) {
    <span class="keyword">int32_t*</span> vector = <span class="function">calloc</span>(<span class="number">30</span>, <span class="keyword">sizeof</span>(<span class="keyword">int32_t</span>));

    FILE* f = <span class="function">fopen</span>(tokenizer_file, <span class="string">"r"</span>);
    <span class="keyword">if</span> (!f) <span class="keyword">return</span> NULL;
    
    <span class="function">fseek</span>(f, 0, SEEK_END);
    <span class="keyword">long</span> len = <span class="function">ftell</span>(f);
    <span class="function">fseek</span>(f, 0, SEEK_SET);
    <span class="keyword">char*</span> json_str = <span class="function">malloc</span>(len + <span class="number">1</span>);
    <span class="function">fread</span>(json_str, 1, len, f);
    json_str[len] = 0;
    <span class="function">fclose</span>(f);

    cJSON* tokenizer = <span class="function">cJSON_Parse</span>(json_str);
    <span class="keyword">if</span> (!tokenizer) {
        <span class="function">free</span>(json_str);
        <span class="keyword">return</span> NULL;
    }

    <span class="keyword">char*</span> text_copy = <span class="function">strdup</span>(text);
    <span class="keyword">for</span> (<span class="keyword">char*</span> p = text_copy; *p; p++) *p = <span class="function">tolower</span>(*p);

    <span class="keyword">char*</span> word = <span class="function">strtok</span>(text_copy, <span class="string">" \t\n"</span>);
    <span class="keyword">int</span> idx = 0;
    <span class="keyword">while</span> (word &amp;&amp; idx < 30) {
        cJSON* token = <span class="function">cJSON_GetObjectItem</span>(tokenizer, word);
        vector[idx++] = token ? token->valueint : (<span class="function">cJSON_GetObjectItem</span>(tokenizer, <span class="string">"&lt;OOV&gt;"</span>) ? <span class="function">cJSON_GetObjectItem</span>(tokenizer, <span class="string">"&lt;OOV&gt;"</span>)->valueint : 1);
        word = <span class="function">strtok</span>(NULL, <span class="string">" \t\n"</span>);
    }

    <span class="function">free</span>(text_copy);
    <span class="function">free</span>(json_str);
    <span class="function">cJSON_Delete</span>(tokenizer);
    <span class="keyword">return</span> vector;
}

<span class="keyword">int</span> <span class="function">main</span>() {
    g_ort = OrtGetApiBase()->GetApi(ORT_API_VERSION);
    <span class="keyword">if</span> (!g_ort) <span class="keyword">return</span> 1;

    <span class="keyword">const char*</span> text = <span class="string">"That's really thoughtful feedback ‚Äî thank you."</span>;
    <span class="keyword">int32_t*</span> vector = <span class="function">preprocess_text</span>(text, <span class="string">"hate_speech(English)/vocab.json"</span>);
    <span class="keyword">if</span> (!vector) <span class="keyword">return</span> 1;

    OrtEnv* env;
    OrtStatus* status = g_ort->CreateEnv(ORT_LOGGING_LEVEL_WARNING, <span class="string">"test"</span>, &env);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    OrtSessionOptions* session_options;
    status = g_ort->CreateSessionOptions(&session_options);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    OrtSession* session;
    status = g_ort->CreateSession(env, <span class="string">"hate_speech(English)/model.onnx"</span>, session_options, &session);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    OrtMemoryInfo* memory_info;
    status = g_ort->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    <span class="keyword">int64_t</span> input_shape[] = {1, 30};
    OrtValue* input_tensor;
    status = g_ort->CreateTensorWithDataAsOrtValue(memory_info, vector, 30 * <span class="keyword">sizeof</span>(<span class="keyword">int32_t</span>), input_shape, 2, ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32, &input_tensor);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    <span class="keyword">const char*</span> input_names[] = {<span class="string">"input"</span>};
    <span class="keyword">const char*</span> output_names[] = {<span class="string">"sequential"</span>};
    OrtValue* output_tensor = NULL;
    status = g_ort->Run(session, NULL, input_names, (<span class="keyword">const</span> OrtValue* <span class="keyword">const</span>*)&input_tensor, 1, output_names, 1, &output_tensor);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    <span class="keyword">float*</span> output_data;
    status = g_ort->GetTensorMutableData(output_tensor, (<span class="keyword">void</span>**)&output_data);
    <span class="keyword">if</span> (status) <span class="keyword">return</span> 1;

    FILE* f = <span class="function">fopen</span>(<span class="string">"hate_speech(English)/scaler.json"</span>, <span class="string">"r"</span>);
    <span class="keyword">if</span> (!f) <span class="keyword">return</span> 1;

    <span class="function">fseek</span>(f, 0, SEEK_END);
    <span class="keyword">long</span> len = <span class="function">ftell</span>(f);
    <span class="function">fseek</span>(f, 0, SEEK_SET);
    <span class="keyword">char*</span> json_str = <span class="function">malloc</span>(len + <span class="number">1</span>);
    <span class="function">fread</span>(json_str, 1, len, f);
    json_str[len] = 0;
    <span class="function">fclose</span>(f);

    cJSON* label_map = <span class="function">cJSON_Parse</span>(json_str);
    <span class="keyword">if</span> (!label_map) {
        <span class="function">free</span>(json_str);
        <span class="keyword">return</span> 1;
    }

    <span class="keyword">int</span> predicted_idx = 0;
    <span class="keyword">float</span> max_prob = output_data[0];
    <span class="keyword">int</span> num_classes = <span class="function">cJSON_GetArraySize</span>(label_map);
    <span class="keyword">for</span> (<span class="keyword">int</span> i = 1; i < num_classes; i++) {
        <span class="keyword">if</span> (output_data[i] > max_prob) {
            max_prob = output_data[i];
            predicted_idx = i;
        }
    }

    <span class="keyword">char</span> idx_str[16];
    <span class="function">snprintf</span>(idx_str, <span class="keyword">sizeof</span>(idx_str), <span class="string">"%d"</span>, predicted_idx);
    cJSON* label = <span class="function">cJSON_GetObjectItem</span>(label_map, idx_str);
    <span class="keyword">if</span> (!label) <span class="keyword">return</span> 1;
    
    <span class="function">printf</span>(<span class="string">"C ONNX output: %s (Score: %.4f)\n"</span>, label->valuestring, max_prob);

    g_ort->ReleaseValue(input_tensor);
    g_ort->ReleaseValue(output_tensor);
    g_ort->ReleaseMemoryInfo(memory_info);
    g_ort->ReleaseSession(session);
    g_ort->ReleaseSessionOptions(session_options);
    g_ort->ReleaseEnv(env);

    <span class="function">free</span>(vector);
    <span class="function">free</span>(json_str);
    <span class="function">cJSON_Delete</span>(label_map);

    <span class="keyword">return</span> 0;
}

<span class="output">C ONNX output: Not_hate (Score: 0.9971)</span></code></pre>

                                <h4>‚úÖ 7. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>C ONNX output: Not_hate (Score: 0.9971)</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="code-block" id="cpp-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with C++ using ONNX Runtime and nlohmann/json</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide explains how to load and run ONNX models using C++, ONNX Runtime C++ API, and nlohmann/json for JSON parsing.</p>

                                <h4>‚úÖ 1. Prerequisites</h4>
                                <div class="os-section">
                                    <h5>üî∑ C++ Compiler</h5>
                                    <p>macOS: clang++ comes with Xcode Command Line Tools</p>
                                    <pre><code>xcode-select --install</code></pre>
                                    <p>Linux: install g++</p>
                                    <pre><code>sudo apt install build-essential</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üü© ONNX Runtime C++ Library</h5>
                                    <p>Download ONNX Runtime C++ API from the official website:</p>
                                    <p>üëâ <a href="https://github.com/microsoft/onnxruntime/releases" class="neon-link">https://github.com/microsoft/onnxruntime/releases</a></p>
                                    <p>Choose:</p>
                                    <pre><code>onnxruntime-osx-universal2-<version>.tgz   # For macOS
onnxruntime-linux-x64-<version>.tgz        # For Linux</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Install nlohmann/json</h5>
                                    <p>macOS:</p>
                                    <pre><code>brew install nlohmann-json</code></pre>
                                    <p>Linux:</p>
                                    <pre><code>sudo apt install nlohmann-json3-dev</code></pre>
                                </div>

                                <h4>‚úÖ 2. Choose Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 3. Folder Structure</h4>
                                <pre><code>project/
‚îú‚îÄ‚îÄ main.cpp                ‚Üê your C++ code
‚îú‚îÄ‚îÄ vocab.json             ‚Üê tokenizer
‚îú‚îÄ‚îÄ scaler.json            ‚Üê label map
‚îú‚îÄ‚îÄ model.onnx             ‚Üê ONNX model
‚îú‚îÄ‚îÄ onnxruntime-osx-universal2-1.22.0/
‚îÇ   ‚îú‚îÄ‚îÄ include/
‚îÇ   ‚îî‚îÄ‚îÄ lib/</code></pre>

                                <h4>‚úÖ 4. Build Command</h4>
                                <div class="os-section">
                                    <h5>üî∑ macOS</h5>
                                    <pre><code>g++ -std=c++17 main.cpp \
  -I./onnxruntime-osx-universal2-1.22.0/include \
  -L./onnxruntime-osx-universal2-1.22.0/lib \
  -lonnxruntime \
  -o onnx_test</code></pre>
                                </div>
                                <div class="os-section">
                                    <h5>üêß Linux</h5>
                                    <p>Replace the onnxruntime-osx-... path with onnxruntime-linux-x64-....</p>
                                </div>

                                <h4>‚úÖ 5. Run the Executable</h4>
                                <div class="os-section">
                                    <h5>üî∑ macOS</h5>
                                    <p>Important: You must set the library path.</p>
                                    <pre><code>export DYLD_LIBRARY_PATH=./onnxruntime-osx-universal2-1.22.0/lib:$DYLD_LIBRARY_PATH
./onnx_test</code></pre>
                                </div>
                                <div class="os-section">
                                    <h5>üêß Linux</h5>
                                    <pre><code>export LD_LIBRARY_PATH=./onnxruntime-linux-x64-1.22.0/lib:$LD_LIBRARY_PATH
./onnx_test</code></pre>
                                </div>

                                <h4>‚úÖ 6. C++ Code Example</h4>
                                <pre><code><span class="keyword">#include</span> <span class="string">&lt;onnxruntime_cxx_api.h&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;fstream&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;nlohmann/json.hpp&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;iostream&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;iomanip&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;algorithm&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;string&gt;</span>
<span class="keyword">#include</span> <span class="string">&lt;vector&gt;</span>

<span class="keyword">using</span> <span class="class">json</span> = nlohmann::json;

<span class="keyword">std::vector&lt;int32_t&gt;</span> <span class="function">preprocess_text</span>(<span class="parameter">const std::string&amp; text</span>, <span class="parameter">const std::string&amp; tokenizer_file</span>) {
    <span class="class">std::vector&lt;int32_t&gt;</span> vector(30, 0);
    
    <span class="class">std::ifstream</span> tf(tokenizer_file);
    json tokenizer; tf >> tokenizer;
    
    <span class="class">std::string</span> text_lower = text;
    std::transform(text_lower.begin(), text_lower.end(), text_lower.begin(), ::tolower);
    <span class="class">std::vector&lt;std::string&gt;</span> words;
    size_t start = 0, end;
    <span class="keyword">while</span> ((end = text_lower.find(' ', start)) != std::string::npos) {
        <span class="keyword">if</span> (end > start) words.push_back(text_lower.substr(start, end - start));
        start = end + 1;
    }
    <span class="keyword">if</span> (start < text_lower.length()) words.push_back(text_lower.substr(start));
    
    <span class="keyword">for</span> (size_t i = 0; i < std::min(words.size(), size_t(30)); i++) {
        auto it = tokenizer.find(words[i]);
        <span class="keyword">if</span> (it != tokenizer.end()) {
            vector[i] = it->get<int>();
        } <span class="keyword">else</span> {
            auto oov = tokenizer.find("<OOV>");
            vector[i] = oov != tokenizer.end() ? oov->get<int>() : 1;
        }
    }
    <span class="keyword">return</span> vector;
}

<span class="keyword">int</span> <span class="function">main</span>() {
    <span class="class">std::string</span> text = <span class="string">"I hate you"</span>;
    auto vector = <span class="function">preprocess_text</span>(text, <span class="string">"hate_speech(English)/vocab.json"</span>);
    
    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, <span class="string">"test"</span>);
    Ort::SessionOptions session_options;
    Ort::Session session(env, <span class="string">"hate_speech(English)/model.onnx"</span>, session_options);
    
    <span class="class">std::vector&lt;int64_t&gt;</span> input_shape = {1, 30};
    Ort::MemoryInfo memory_info(<span class="string">"Cpu"</span>, OrtDeviceAllocator, 0, OrtMemTypeDefault);
    Ort::Value input_tensor = Ort::Value::CreateTensor(memory_info, vector.data(), vector.size(), 
                                                     input_shape.data(), input_shape.size());
    
    <span class="class">std::vector&lt;const char*&gt;</span> input_names = {<span class="string">"input"</span>};
    <span class="class">std::vector&lt;const char*&gt;</span> output_names = {<span class="string">"sequential"</span>};
    auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_names.data(), &input_tensor, 1, 
                                    output_names.data(), 1);
    
    float* output_data = output_tensors[0].GetTensorMutableData<float>();
    size_t output_size = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();
    
    <span class="class">std::ifstream</span> lf(<span class="string">"hate_speech(English)/scaler.json"</span>);
    json label_map; lf >> label_map;
    
    auto max_it = std::max_element(output_data, output_data + output_size);
    int predicted_idx = std::distance(output_data, max_it);
    <span class="class">std::string</span> label = label_map[std::to_string(predicted_idx)];
    float score = *max_it;
    
    std::cout << <span class="string">"C++ ONNX output: "</span> << label << <span class="string">" (Score: "</span> << std::fixed << std::setprecision(4) 
              << score << <span class="string">")"</span> << std::endl;
    <span class="keyword">return</span> 0;
}

<span class="output">C++ ONNX output: Hate (Score: 0.9876)</span></code></pre>

                                <h4>‚úÖ 7. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>C++ ONNX output: Hate (Score: 0.9876)</code></pre>
                                </div>
                            </div>
                        </div>
                        <div class="code-block" id="rust-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with Rust: Full Beginner-Friendly Guide</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide walks you through running an ONNX model for text classification using Rust, starting from scratch ‚Äî including Rust installation, setting up dependencies, and running the model.</p>

                                <h4>‚úÖ 1. Install Rust</h4>
                                <div class="os-section">
                                    <h5>üî∑ Windows</h5>
                                    <ol>
                                        <li>Download and run rustup-init.exe from: <a href="https://rustup.rs/" class="neon-link">https://rustup.rs/</a></li>
                                        <li>Follow the installation prompts</li>
                                        <li>After installation, open a new terminal and verify:</li>
                                    </ol>
                                    <pre><code>rustc --version
cargo --version</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üçè macOS</h5>
                                    <p>Open Terminal and run:</p>
                                    <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</code></pre>
                                    <p>After installation, verify:</p>
                                    <pre><code>rustc --version
cargo --version</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üêß Linux</h5>
                                    <p>Open Terminal and run:</p>
                                    <pre><code>curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</code></pre>
                                    <p>After installation, verify:</p>
                                    <pre><code>rustc --version
cargo --version</code></pre>
                                </div>

                                <h4>‚úÖ 2. Get Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 3. Create a New Rust Project</h4>
                                <pre><code>cargo new onnx_rust_demo
cd onnx_rust_demo</code></pre>

                                <h4>‚úÖ 4. Add Dependencies</h4>
                                <p>Edit <code>Cargo.toml</code> to add required dependencies:</p>
                                <pre><code>[package]
name = "onnx_rust_demo"
version = "0.1.0"
edition = "2021"

[dependencies]
ort = "1.16.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
thiserror = "1.0"</code></pre>

                                <h4>‚úÖ 5. Project Structure</h4>
                                <pre><code>onnx_rust_demo/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ main.rs
‚îú‚îÄ‚îÄ model.onnx
‚îú‚îÄ‚îÄ vocab.json
‚îú‚îÄ‚îÄ scaler.json
‚îî‚îÄ‚îÄ Cargo.toml</code></pre>

                                <h4>‚úÖ 6. Create Supporting Files</h4>
                                <div class="file-section">
                                    <h5>üîπ vocab.json (tokenizer dictionary)</h5>
                                    <pre><code>{
  "<OOV>": 1,
  "the": 2,
  "government": 3,
  "announced": 4,
  "new": 5,
  "policies": 6,
  "to": 7,
  "boost": 8,
  "economy": 9
}</code></pre>
                                </div>

                                <div class="file-section">
                                    <h5>üîπ scaler.json (label map)</h5>
                                    <pre><code>{
  "0": "Politics",
  "1": "Sports",
  "2": "Technology"
}</code></pre>
                                </div>

                                <h4>‚úÖ 7. Create the Rust Code (src/main.rs)</h4>
                                <pre><code><span class="keyword">use</span> <span class="module">anyhow</span>::<span class="class">Result</span>;
<span class="keyword">use</span> <span class="module">ort</span>::{<span class="class">Environment</span>, <span class="class">Session</span>, <span class="class">SessionBuilder</span>, <span class="class">Value</span>};
<span class="keyword">use</span> <span class="module">serde_json</span>::<span class="class">Value</span> <span class="keyword">as</span> <span class="class">JsonValue</span>;
<span class="keyword">use</span> <span class="module">std</span>::fs::File;
<span class="keyword">use</span> <span class="module">std</span>::io::Read;
<span class="keyword">use</span> <span class="module">std</span>::collections::HashMap;

<span class="keyword">struct</span> <span class="class">Tokenizer</span> {
    vocab: <span class="class">HashMap&lt;String, i32&gt;</span>,
}

<span class="keyword">impl</span> <span class="class">Tokenizer</span> {
    <span class="keyword">fn</span> <span class="function">new</span>(<span class="parameter">vocab_file: &str</span>) -> <span class="class">Result&lt;Self&gt;</span> {
        <span class="keyword">let mut</span> file = File::open(vocab_file)?;
        <span class="keyword">let mut</span> contents = String::new();
        file.read_to_string(&mut contents)?;
        <span class="keyword">let</span> vocab: <span class="class">HashMap&lt;String, i32&gt;</span> = serde_json::from_str(&contents)?;
        <span class="keyword">Ok</span>(<span class="class">Tokenizer</span> { vocab })
    }

    <span class="keyword">fn</span> <span class="function">tokenize</span>(&self, <span class="parameter">text: &str</span>) -> <span class="class">Vec&lt;i32&gt;</span> {
        <span class="keyword">let</span> words: <span class="class">Vec&lt;&str&gt;</span> = text.to_lowercase().split_whitespace().collect();
        <span class="keyword">let mut</span> tokens = <span class="class">Vec</span>::with_capacity(30);
        
        <span class="keyword">for</span> word <span class="keyword">in</span> words.iter().take(30) {
            <span class="keyword">let</span> token = self.vocab.get(*word)
                .or_else(|| self.vocab.get("<OOV>"))
                .copied()
                .unwrap_or(1);
            tokens.push(token);
        }

        <span class="keyword">while</span> tokens.len() < 30 {
            tokens.push(0);
        }

        tokens
    }
}

<span class="keyword">fn</span> <span class="function">load_label_map</span>(<span class="parameter">file_path: &str</span>) -> <span class="class">Result&lt;HashMap&lt;String, String&gt;&gt;</span> {
    <span class="keyword">let mut</span> file = File::open(file_path)?;
    <span class="keyword">let mut</span> contents = String::new();
    file.read_to_string(&mut contents)?;
    <span class="keyword">let</span> label_map: <span class="class">HashMap&lt;String, String&gt;</span> = serde_json::from_str(&contents)?;
    <span class="keyword">Ok</span>(label_map)
}

<span class="keyword">fn</span> <span class="function">main</span>() -> <span class="class">Result&lt;()&gt;</span> {
    <span class="comment">// Initialize tokenizer and load label map</span>
    <span class="keyword">let</span> tokenizer = <span class="class">Tokenizer</span>::new("vocab.json")?;
    <span class="keyword">let</span> label_map = load_label_map("scaler.json")?;

    <span class="comment">// Create ONNX Runtime environment and session</span>
    <span class="keyword">let</span> environment = <span class="class">Environment</span>::builder()
        .with_name("onnx-rust-demo")
        .build()?;
    
    <span class="keyword">let</span> session = <span class="class">SessionBuilder</span>::new(&environment)?
        .with_model_from_file("model.onnx")?;

    <span class="comment">// Prepare input text</span>
    <span class="keyword">let</span> text = "The government announced new policies to boost the economy";
    <span class="keyword">let</span> tokens = tokenizer.tokenize(text);

    <span class="comment">// Create input tensor</span>
    <span class="keyword">let</span> input_tensor = <span class="class">Value</span>::from_array(([1, 30], tokens))?;

    <span class="comment">// Run inference</span>
    <span class="keyword">let</span> outputs = session.run(vec![input_tensor])?;
    <span class="keyword">let</span> output: &<span class="class">Value</span> = outputs[0].downcast_ref().unwrap();
    <span class="keyword">let</span> probabilities = output.as_slice::&lt;f32&gt;()?;

    <span class="comment">// Find predicted class</span>
    <span class="keyword">let</span> (predicted_idx, &score) = probabilities.iter().enumerate()
        .max_by(|a, b| a.1.partial_cmp(b.1).unwrap())
        .unwrap();

    <span class="keyword">let</span> label = label_map.get(&predicted_idx.to_string())
        .ok_or_else(|| anyhow::anyhow!("Label not found"))?;

    println!("Rust ONNX output: {} (Score: {:.4})", label, score);
    <span class="keyword">Ok</span>(())
}

<span class="output">Rust ONNX output: Politics (Score: 0.9123)</span></code></pre>

                                <h4>‚úÖ 8. Build and Run</h4>
                                <pre><code>cargo build --release
cargo run --release</code></pre>

                                <h4>‚úÖ 9. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>Rust ONNX output: Politics (Score: 0.9123)</code></pre>
                                </div>

                                <h4>‚úÖ 10. Troubleshooting</h4>
                                <div class="troubleshooting-section">
                                    <h5>Common Issues and Solutions:</h5>
                                    <ul>
                                        <li><strong>Error: "Could not find ONNX Runtime"</strong>
                                            <p>Solution: Make sure you have the correct version of ONNX Runtime installed and the library path is set correctly.</p>
                                        </li>
                                        <li><strong>Error: "Failed to load model"</strong>
                                            <p>Solution: Verify that the model file path is correct and the file exists.</p>
                                        </li>
                                        <li><strong>Error: "Invalid input shape"</strong>
                                            <p>Solution: Ensure your input tensor has the correct shape [1, 30] and contains valid token IDs.</p>
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                        <div class="code-block" id="java-code">
                            <div class="guide-section">
                                <h3 class="neon-green">üß† How to Run an ONNX Model with Java using ONNX Runtime and org.json</h3>
                                
                                <h4>üìå What is this?</h4>
                                <p>This guide explains how to load and run ONNX models using Java, ONNX Runtime Java API, and org.json for JSON parsing.</p>

                                <h4>‚úÖ 1. Prerequisites</h4>
                                <div class="os-section">
                                    <h5>üî∑ Java Development Kit (JDK)</h5>
                                    <p>Install JDK 11 or later:</p>
                                    <ol>
                                        <li>Download from: <a href="https://adoptium.net/" class="neon-link">https://adoptium.net/</a></li>
                                        <li>Verify installation:</li>
                                    </ol>
                                    <pre><code>java -version
javac -version</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üü© Maven</h5>
                                    <p>Install Maven for dependency management:</p>
                                    <ol>
                                        <li>Download from: <a href="https://maven.apache.org/download.cgi" class="neon-link">https://maven.apache.org/download.cgi</a></li>
                                        <li>Verify installation:</li>
                                    </ol>
                                    <pre><code>mvn -version</code></pre>
                                </div>

                                <h4>‚úÖ 2. Choose Your Model</h4>
                                <div class="os-section">
                                    <h5>üîÑ Download Repository</h5>
                                    <p>Clone our repository to get started:</p>
                                    <pre><code>git clone https://github.com/volodymyrparanyak/whitelightning.ai.git
cd whitelightning.ai</code></pre>
                                </div>

                                <div class="os-section">
                                    <h5>üì¶ Choose Your Model</h5>
                                    <p>You have two options:</p>
                                    <ol>
                                        <li><strong>Use Pre-trained Model:</strong>
                                            <ul>
                                                <li>Navigate to the <code>models</code> directory</li>
                                                <li>Copy the multiclass classifier model files to your project:
                                                    <ul>
                                                        <li><code>model.onnx</code> - The ONNX model file</li>
                                                        <li><code>model_vocab.json</code> - Tokenizer vocabulary</li>
                                                        <li><code>model_labels.json</code> - Class labels mapping</li>
                                                    </ul>
                                                </li>
                                            </ul>
                                        </li>
                                        <li><strong>Train Your Own Model:</strong>
                                            <ul>
                                                <li>Follow the training guide in the repository</li>
                                                <li>Use the provided scripts to train a custom multiclass classifier</li>
                                                <li>Export your model to ONNX format</li>
                                            </ul>
                                        </li>
                                    </ol>
                                </div>

                                <h4>‚úÖ 3. Create a New Maven Project</h4>
                                <pre><code>mvn archetype:generate -DgroupId=com.example -DartifactId=onnx-java-demo -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false
cd onnx-java-demo</code></pre>

                                <h4>‚úÖ 4. Add Dependencies</h4>
                                <p>Edit <code>pom.xml</code> to add required dependencies:</p>
                                <pre><code>&lt;dependencies&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;com.microsoft.onnxruntime&lt;/groupId&gt;
        &lt;artifactId&gt;onnxruntime&lt;/artifactId&gt;
        &lt;version&gt;1.16.3&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;dependency&gt;
        &lt;groupId&gt;org.json&lt;/groupId&gt;
        &lt;artifactId&gt;json&lt;/artifactId&gt;
        &lt;version&gt;20231013&lt;/version&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre>

                                <h4>‚úÖ 5. Project Structure</h4>
                                <pre><code>onnx-java-demo/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ main/
‚îÇ       ‚îú‚îÄ‚îÄ java/
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ com/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ example/
‚îÇ       ‚îÇ           ‚îî‚îÄ‚îÄ ONNXModelRunner.java
‚îÇ       ‚îî‚îÄ‚îÄ resources/
‚îÇ           ‚îú‚îÄ‚îÄ model.onnx
‚îÇ           ‚îú‚îÄ‚îÄ vocab.json
‚îÇ           ‚îî‚îÄ‚îÄ scaler.json
‚îî‚îÄ‚îÄ pom.xml</code></pre>

                                <h4>‚úÖ 6. Java Code Example</h4>
                                <pre><code><span class="keyword">import</span> <span class="module">ai.onnxruntime.*</span>;
<span class="keyword">import</span> <span class="module">org.json.JSONObject</span>;
<span class="keyword">import</span> <span class="module">java.nio.file.Files</span>;
<span class="keyword">import</span> <span class="module">java.nio.file.Paths</span>;
<span class="keyword">import</span> <span class="module">java.util.*</span>;

<span class="keyword">public class</span> <span class="class">ONNXModelRunner</span> {
    <span class="keyword">public static void</span> <span class="function">main</span>(<span class="parameter">String[] args</span>) {
        <span class="keyword">try</span> {
            <span class="class">LabelVocabLoader</span> loader = <span class="keyword">new</span> <span class="class">LabelVocabLoader</span>(<span class="string">"resources/labelMap.json"</span>, <span class="string">"resources/vocab.json"</span>);
            Map&lt;Integer, String&gt; labelMap = loader.getLabelMap();
            Map&lt;String, Integer&gt; vocab = loader.getVocab();

            String modelPath = <span class="string">"resources/model.onnx"</span>;
            OrtEnvironment env = OrtEnvironment.getEnvironment();
            OrtSession session = env.createSession(modelPath, <span class="keyword">new</span> OrtSession.SessionOptions());

            String inputText = <span class="string">"The government announced new policies to boost the economy"</span>;

            Tokenizer tokenizer = <span class="keyword">new</span> Tokenizer(vocab);
            int maxLen = <span class="number">30</span>;
            int[] tokenizedInput = tokenizer.tokenize(inputText);
            int[] paddedInput = <span class="keyword">new</span> int[maxLen];
            <span class="keyword">for</span> (int i = 0; i < maxLen; i++) {
                <span class="keyword">if</span> (i < tokenizedInput.length) {
                    paddedInput[i] = tokenizedInput[i];
                } <span class="keyword">else</span> {
                    paddedInput[i] = 0;
                }
            }

            int[][] inputData = <span class="keyword">new</span> int[1][maxLen];
            inputData[0] = paddedInput;

            OnnxTensor inputTensor = OnnxTensor.createTensor(env, inputData);

            String inputName = session.getInputNames().iterator().next();
            OrtSession.Result result = session.run(Collections.singletonMap(inputName, inputTensor));

            float[][] outputArray = (float[][]) result.get(0).getValue();
            int maxIndex = 0;
            float maxScore = outputArray[0][0];
            <span class="keyword">for</span> (int i = 1; i < outputArray[0].length; i++) {
                <span class="keyword">if</span> (outputArray[0][i] > maxScore) {
                    maxScore = outputArray[0][i];
                    maxIndex = i;
                }
            }

            System.out.println(<span class="string">"Java ONNX output: "</span> + labelMap.get(maxIndex) + 
                             <span class="string">" (Score: "</span> + String.format("%.4f", maxScore) + <span class="string">")"</span>);

            session.close();
            env.close();
        } <span class="keyword">catch</span> (Exception e) {
            e.printStackTrace();
        }
    }

    <span class="keyword">static class</span> <span class="class">Tokenizer</span> {
        <span class="keyword">private</span> Map&lt;String, Integer&gt; vocab;

        <span class="keyword">public</span> <span class="function">Tokenizer</span>(Map&lt;String, Integer&gt; vocab) {
            this.vocab = vocab;
        }

        <span class="keyword">public int[]</span> <span class="function">tokenize</span>(String text) {
            String[] words = text.toLowerCase().split(<span class="string">"\\s+"</span>);
            int[] tokenized = <span class="keyword">new</span> int[words.length];
            <span class="keyword">for</span> (int i = 0; i < words.length; i++) {
                Integer token = vocab.getOrDefault(words[i], vocab.get(<span class="string">"<OOV>"</span>));
                tokenized[i] = token;
            }
            return tokenized;
        }
    }

    <span class="keyword">static class</span> <span class="class">LabelVocabLoader</span> {
        <span class="keyword">private</span> Map&lt;Integer, String&gt; labelMap;
        <span class="keyword">private</span> Map&lt;String, Integer&gt; vocab;

        <span class="keyword">public</span> <span class="function">LabelVocabLoader</span>(String labelMapPath, String vocabPath) <span class="keyword">throws</span> Exception {
            String labelMapJson = <span class="keyword">new</span> String(Files.readAllBytes(Paths.get(labelMapPath)));
            JSONObject labelMapObject = <span class="keyword">new</span> JSONObject(labelMapJson);
            this.labelMap = <span class="keyword">new</span> HashMap<>();
            <span class="keyword">for</span> (String key : labelMapObject.keySet()) {
                this.labelMap.put(Integer.parseInt(key), labelMapObject.getString(key));
            }

            String vocabJson = <span class="keyword">new</span> String(Files.readAllBytes(Paths.get(vocabPath)));
            JSONObject vocabObject = <span class="keyword">new</span> JSONObject(vocabJson);
            this.vocab = <span class="keyword">new</span> HashMap<>();
            <span class="keyword">for</span> (String key : vocabObject.keySet()) {
                this.vocab.put(key, vocabObject.getInt(key));
            }
        }

        <span class="keyword">public</span> Map&lt;Integer, String&gt; <span class="function">getLabelMap</span>() {
            return labelMap;
        }

        <span class="keyword">public</span> Map&lt;String, Integer&gt; <span class="function">getVocab</span>() {
            return vocab;
        }
    }
}</code></pre>

                                <h4>‚úÖ 7. Build and Run</h4>
                                <pre><code>mvn clean package
java -cp target/onnx-java-demo-1.0-SNAPSHOT.jar com.example.ONNXModelRunner</code></pre>

                                <h4>‚úÖ 8. Expected Output</h4>
                                <div class="output-section">
                                    <pre><code>Java ONNX output: Politics (Score: 0.9123)</code></pre>
                                </div>
                            </div>
                        </div>
                        <!-- Add other language code blocks similarly -->
                    </div>
                </div>
            </section>
        </main>
    </div>

    <footer class="site-footer">
        <div class="footer-content container">
          <div class="footer-brand">
            <img src="media/image/moonshiner_floppy.jpeg" alt="WhiteLightning Logo" class="footer-logo">
            <div>
              <div class="footer-title">WhiteLightning</div>
              <div class="footer-desc">Create Edge‚ÄëReady AI in One Line No Data Required</div>
            </div>
          </div>
          <div class="footer-columns">
            <div class="footer-col">
              <div class="footer-col-title">Docs & Links</div>
              <a href="docs.html">Documentation</a>
              <a href="playground.html">Playground</a>
              <a href="https://github.com/volodymyrparanyak/whitelightning.ai" target="_blank">GitHub</a>
            </div>
            <div class="footer-col">
              <div class="footer-col-title">Resources</div>
              <a href="docs.html#features">Features</a>
              <a href="demo_mlt.html">Demo</a>
              <!-- <a href="#">ONNX Format</a> -->
            </div>
            <div class="footer-col">
              <div class="footer-col-title">Project</div>
              <a href="https://github.com/volodymyrparanyak/whitelightning.ai/pulls" target="_blank">Report Issues</a>
              <a href="https://github.com/volodymyrparanyak/whitelightning.ai/blob/alpha-0.0.1/LICENSE" target="_blank">License</a>
            </div>
          </div>
        </div>
        <div class="footer-bottom container">
          <span>2025 WhiteLightning Project</span>
          <span>Apache License 2.0</span>
          <span>contact@whitelightning.ai</span>
        </div>
      </footer>

    <script src="scripts/mobile-menu.js"></script>
    <script>
    const typewriterIntroText = `Ready to unleash your WhiteLightning.ai ONNX multiclass models? These snippets show how to run them across various languages, from Python to Rust, for classifying text into multiple categories (e.g., Politics, Sports, Business). Each snippet assumes a preprocessed input of 30 integer token IDs, crafted from raw text using a tokenizer. Use WhiteLightning.ai's CLI to prepare your data; below, we break down the process for smooth deployment.`;
    const typewriterIntroElem = document.getElementById('typewriter-intro');
    let introIdx = 0;
    function typeWriterIntro() {
        if (introIdx < typewriterIntroText.length) {
            typewriterIntroElem.innerHTML += typewriterIntroText.charAt(introIdx);
            introIdx++;
            setTimeout(typeWriterIntro, 18);
        }
    }
    typeWriterIntro();

    document.querySelector('.copy-btn').addEventListener('click', function() {
      // Find the active code block
      const activeBlock = document.querySelector('.code-block.active code');
      if (!activeBlock) return;
      let text = activeBlock.innerText || activeBlock.textContent;
      text = text.replace(/‚ñã$/, ''); // Remove cursor if present
      navigator.clipboard.writeText(text).then(() => {
        const btn = document.querySelector('.copy-btn');
        btn.textContent = 'Copied!';
        setTimeout(() => btn.textContent = 'Copy', 1200);
      });
    });
    </script>
</body>
</html> 